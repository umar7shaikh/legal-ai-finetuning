# -*- coding: utf-8 -*-
"""Legal_AI_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUSEIbDBJvgAbRaLViX727_LBBf1gmSU
"""

# Step1: Create & Setup hugging face API token in Collab

!pip install unsloth # install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

# Step3: Import necessary libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from google.colab import userdata
hf_token = userdata.get('HF_TOKEN')
login(hf_token)

# Optional: Check GPU availability
# Test if CUDA is available
import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
max_sequence_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_sequence_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    token = hf_token
)

# Step6: Setup system prompt (Legal Version)
prompt_style = """
Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.

Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.

### Task:
You are a legal expert specializing in Indian Constitution, IPC, and CrPC. Answer the legal question below using your advanced knowledge.

### Query:
{}

### Answer:
{}
"""

# Step7: Run Inference on the model (Legal Version)

# Define a test legal question
question = """Under what circumstances can a person be granted anticipatory bail under CrPC?"""

# Prepare the model for inference
FastLanguageModel.for_inference(model)

# Tokenize the input using the original prompt style
inputs = tokenizer(
    [prompt_style.format(question, "")],
    return_tensors="pt"
).to("cuda")

# Generate a response (keeping original parameters)
outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True
)

# Decode and print the full response
response = tokenizer.batch_decode(outputs)[0]
print(response)

print(response[0].split("### Answer")[1])

!git clone https://huggingface.co/datasets/Techmaestro369/indian-legal-texts-finetuning

import json
from datasets import Dataset

def load_json_to_dataset(file_path):
    with open(file_path, "r") as f:
        data = json.load(f)
    return Dataset.from_list(data)

# Load each JSON file
constitution = load_json_to_dataset("indian-legal-texts-finetuning/constitution_qa.json")
ipc = load_json_to_dataset("indian-legal-texts-finetuning/ipc_qa.json")
crpc = load_json_to_dataset("indian-legal-texts-finetuning/crpc_qa.json")

import json
from datasets import Dataset

def load_json_to_dataset(file_path):
    with open(file_path, "r") as f:
        data = json.load(f)
    return Dataset.from_list(data)

# Load each JSON file
constitution = load_json_to_dataset("indian-legal-texts-finetuning/constitution_qa.json")
ipc = load_json_to_dataset("indian-legal-texts-finetuning/ipc_qa.json")
crpc = load_json_to_dataset("indian-legal-texts-finetuning/crpc_qa.json")

from datasets import concatenate_datasets

legal_dataset = concatenate_datasets([constitution, ipc, crpc])
legal_dataset = legal_dataset.shuffle(seed=42)  # Optional shuffling

print(legal_dataset)
print(legal_dataset[0])  # Check a sample

EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training
EOS_TOKEN

### Finetuning (Legal Version)
# Updated training prompt style with legal focus
train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a legal expert with advanced knowledge of Indian Constitution, IPC, and CrPC.
Please answer the following legal question.

### Question:
{}

### Legal Reasoning:
<think>
{}
</think>

### Response:
{}"""

# Prepare legal data for fine-tuning
def preprocess_input_data(examples):
    inputs = examples["question"]  # Changed from "Question" to match legal dataset
    outputs = examples["answer"]   # Changed from "Response"

    texts = []

    for input, output in zip(inputs, outputs):
        # Generate synthetic legal reasoning if Complex_CoT not available
        cot = f"""1. Jurisdiction: Identify relevant code (IPC/CrPC/Constitution)
2. Key Terms: {', '.join(input.split()[:3])}
3. Applicable Sections: [Relevant sections]
4. Precedent: [Landmark case if known]
5. Conclusion: {output[:50]}..."""  # First 50 chars of answer

        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
        texts.append(text)

    return {"text": texts}  # Changed key to singular "text" for SFTTrainer compatibility

# Corrected fine-tuning dataset preparation
finetune_dataset = legal_dataset.map(
    preprocess_input_data,
    batched=True,
    remove_columns=legal_dataset.column_names  # Clean up original columns
)

# Verify the processed dataset
print(finetune_dataset)
print(finetune_dataset[0]['text'])  # Inspect a sample

# Step9: Setup/Apply LoRA finetuning to the model

model_lora = FastLanguageModel.get_peft_model(
    model = model,
    r = 16,
    target_modules = [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3047,
    use_rslora = False,
    loftq_config = None
)

if hasattr(model, '_unwrapped_old_generate'):
    del model._unwrapped_old_generate

# Corrected SFTTrainer Setup for Legal Fine-Tuning
trainer = SFTTrainer(
    model=model_lora,
    tokenizer=tokenizer,
    train_dataset=finetune_dataset,
    dataset_text_field="text",  # Changed from "texts" to match your preprocessed data
    max_seq_length=max_sequence_length,
    dataset_num_proc=2,  # Increased from 1 for faster preprocessing
    packing=True,  # Enable for more efficient training

    # Optimized TrainingArguments
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        num_train_epochs=1,
        warmup_steps=5,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",  # Better than linear for legal tasks
        seed=3407,
        output_dir="legal_outputs",  # Changed from generic "outputs"
        report_to="wandb",  # Enable if using Weights & Biases
        save_strategy="steps",
        save_steps=20
    ),
)

# Setup W&B for Legal Fine-Tuning
from google.colab import userdata
import wandb

# 1. Secure Login
wnb_token = userdata.get("WANDB_API_TOKEN")  # Retrieve from Colab secrets
wandb.login(key=wnb_token)

# 2. Initialize Run with Legal-Specific Config
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-on-Indian-Legal-Texts',  # Updated project name
    job_type="legal_finetuning",  # More specific job type
    config={
        "model": "DeepSeek-R1-Llama-8B",
        "task": "Indian Legal Q&A",
        "dataset": "IPC/CrPC/Constitution",
        "peft": "LoRA",
        "batch_size": 2,
        "lr": 2e-4
    }
)

trainer_stats = trainer.train()

wandb.finish()

# Step10: Testing after fine-tuning (Legal Version)
question = """Under what circumstances can anticipatory bail be granted under CrPC?"""

FastLanguageModel.for_inference(model_lora)

# Tokenize using the same prompt style
inputs = tokenizer(
    [prompt_style.format(question, "")],  # Unchanged prompt structure
    return_tensors="pt"
).to("cuda")

# Generate response (same parameters)
outputs = model_lora.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=400,  # Reduced from 1200 for legal answers
    use_cache=True
)

# Decode and print
response = tokenizer.batch_decode(outputs)[0]
print(response.split("### Answer:")[1].strip())  # Same extraction method



